{"cells":[{"cell_type":"markdown","id":"b105582d","metadata":{"id":"b105582d"},"source":["# 1. 데이터 읽어오기"]},{"cell_type":"code","execution_count":null,"id":"7793150d","metadata":{"id":"7793150d","outputId":"3b20d926-024e-4138-da6c-a585dcc0f305","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669310991,"user_tz":-540,"elapsed":2441,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 크기: 187088\n","Examples:\n"," ['Can we forget about the things I said when I was drunk...', \"I didn't mean to call you that\", \"I can't remember what was said\", 'Or what you threw at me Please tell me', 'Please tell me why', 'My car is in the front yard', 'And I am sleeping with my cloths on', 'I came in throught the window... Last night', 'And your... Gone', \"Gone It's no suprise to me I am my own worst enemy\", 'Cuz every now and then I kick the living shit out of me', 'The smoke alarm is going offf and there a cigarette', 'Still buring Please tell me why', 'My car is in the front yard', \"And I'm sleeping with my clothes on\", 'I came in throught the windo last night', 'And your gone', 'Gone Please tell me why', 'My car is in the front yard', 'And I am sleeping with my clothes on']\n"]}],"source":["import glob\n","import os, re \n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","\n","txt_file_path = '/content/data/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담음\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:20])"]},{"cell_type":"markdown","id":"8bc45a79","metadata":{"id":"8bc45a79"},"source":["# 2. 데이터 정제"]},{"cell_type":"code","execution_count":null,"id":"ea8077cf","metadata":{"id":"ea8077cf","outputId":"8dd62148-0510-4d0b-c137-7a1ca342bdfc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669310998,"user_tz":-540,"elapsed":47,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Can we forget about the things I said when I was drunk...\n","I didn't mean to call you that\n","I can't remember what was said\n","Or what you threw at me Please tell me\n","Please tell me why\n","My car is in the front yard\n","And I am sleeping with my cloths on\n","I came in throught the window... Last night\n","And your... Gone\n","Gone It's no suprise to me I am my own worst enemy\n","Cuz every now and then I kick the living shit out of me\n","The smoke alarm is going offf and there a cigarette\n","Still buring Please tell me why\n","My car is in the front yard\n","And I'm sleeping with my clothes on\n","I came in throught the windo last night\n"]}],"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜀\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜀\n","\n","    if idx >15: break   #  토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외\n","        \n","    print(sentence)"]},{"cell_type":"code","execution_count":null,"id":"3af1e2b0","metadata":{"id":"3af1e2b0"},"outputs":[],"source":["# 정규표현식을 이용한 필터링  \n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip()   \n","    # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n","     # 공백 패턴을 만나면 스페이스 1개로 치환\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n","     # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n","\n","    sentence = sentence.strip()\n","    # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여줌\n","    sentence = '<start> ' + sentence + ' <end>'      \n","    \n","    return sentence"]},{"cell_type":"code","execution_count":null,"id":"80cacda9","metadata":{"id":"80cacda9","outputId":"0eb3293b-4917-4ba7-9545-7504c0ea2854","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669312807,"user_tz":-540,"elapsed":1838,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> can we forget about the things i said when i was drunk . . . <end>',\n"," '<start> i didn t mean to call you that <end>',\n"," '<start> i can t remember what was said <end>',\n"," '<start> or what you threw at me please tell me <end>',\n"," '<start> please tell me why <end>',\n"," '<start> my car is in the front yard <end>',\n"," '<start> and i am sleeping with my cloths on <end>',\n"," '<start> i came in throught the window . . . last night <end>',\n"," '<start> and your . . . gone <end>',\n"," '<start> gone it s no suprise to me i am my own worst enemy <end>']"]},"metadata":{},"execution_count":4}],"source":["# 정제된 문장을 모음\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","        \n","    corpus.append(preprocess_sentence(sentence))\n","        \n","corpus[:10]"]},{"cell_type":"code","execution_count":null,"id":"5b95aa92","metadata":{"id":"5b95aa92","outputId":"47d41e93-04c0-46da-b781-60658610a68b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669316670,"user_tz":-540,"elapsed":3871,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[  2  32  23 ...   0   0   0]\n"," [  2   5 343 ...   0   0   0]\n"," [  2   5  32 ...   0   0   0]\n"," ...\n"," [  2   3   0 ...   0   0   0]\n"," [  2   3   0 ...   0   0   0]\n"," [  2   3   0 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fb2c6788790>\n"]}],"source":["# 토큰화\n","\n","def tokenize(corpus):\n","    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=7000,  # 전체 단어의 개수 \n","        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n","        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n","    )\n","    tokenizer.fit_on_texts(corpus)   # 구축한 corpus로부터 Tokenizer가 사전을 자동구축함\n","\n","    # tokenizer를 활용하여 모델에 입력할 데이터셋을 구축\n","    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n","\t\n","    total_data_text = list(tensor)\n","    num_tokens = [len(tokens) for tokens in total_data_text]\n","    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","    maxlen = int(max_tokens)\n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공\n","    # corpus의 가장 긴 문장을 기준으로 시퀀스 길이를 맞춤\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n","                                                           padding='post',\n","                                                           maxlen=maxlen)  \n","\n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"]},{"cell_type":"code","execution_count":null,"id":"7273c683","metadata":{"id":"7273c683","outputId":"8cba146e-a411-4472-a281-e52a8f6573f3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669316671,"user_tz":-540,"elapsed":59,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[  2  32  23 447 113   6 179   5 107  46]\n"," [  2   5 343  15 243  10 154   7  17   3]\n"," [  2   5  32  15 311  40  57 107   3   0]]\n"]}],"source":["print(tensor[:3, :10])  # 단어 사전이 어떻게 구축되었는지 확인"]},{"cell_type":"code","execution_count":null,"id":"f73d2571","metadata":{"id":"f73d2571","outputId":"0838536f-aaea-46aa-f4bd-8b90b90ce50f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669316672,"user_tz":-540,"elapsed":44,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : i\n","6 : the\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"]}],"source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"]},{"cell_type":"code","execution_count":null,"id":"1dd43283","metadata":{"scrolled":true,"id":"1dd43283","outputId":"2b2833c7-95a0-4638-ee17-deb4183a738e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669316672,"user_tz":-540,"elapsed":36,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[  2  32  23 447 113   6 179   5 107  46   5  57 720  20  20  20   3   0\n","   0]\n","[ 32  23 447 113   6 179   5 107  46   5  57 720  20  20  20   3   0   0\n","   0]\n"]}],"source":["#마지막 토큰을 자름\n","src_input = tensor[:, :-1]\n","\n","#앞에 start부분을 자름 \n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0])\n","print(tgt_input[0])"]},{"cell_type":"markdown","id":"d03a4fbe","metadata":{"id":"d03a4fbe"},"source":["# 3. 평가 데이터셋 분리"]},{"cell_type":"code","execution_count":null,"id":"c0d58236","metadata":{"id":"c0d58236"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n","                                                          tgt_input,\n","                                                          test_size=0.2,\n","                                                          shuffle=True, \n","                                                          random_state=34)"]},{"cell_type":"code","execution_count":null,"id":"ab8b6d7c","metadata":{"id":"ab8b6d7c","outputId":"b8ace6c9-34a3-416c-8b02-d11246edaa1a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642669317158,"user_tz":-540,"elapsed":14,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Source Train:  (140599, 19)\n","Target Train:  (140599, 19)\n"]}],"source":["print('Source Train: ', enc_train.shape)\n","print('Target Train: ', dec_train.shape)"]},{"cell_type":"markdown","id":"cb99dbab","metadata":{"id":"cb99dbab"},"source":["# 4. 인공지능 만들기"]},{"cell_type":"code","execution_count":null,"id":"efd4651d","metadata":{"id":"efd4651d"},"outputs":[],"source":["# 텍스트 생성 모델\n","class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super(TextGenerator, self).__init__()\n","        \n","        self.embedding = Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n","        self.linear = Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 19\n","hidden_size = 2048\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","execution_count":null,"id":"5e63814c","metadata":{"scrolled":true,"id":"5e63814c","outputId":"dbeab9c7-7615-402b-b4b5-bac46ab54f55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642672839719,"user_tz":-540,"elapsed":3410932,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","550/550 [==============================] - 352s 610ms/step - loss: 2.9215 - val_loss: 2.5980\n","Epoch 2/10\n","550/550 [==============================] - 339s 617ms/step - loss: 2.4771 - val_loss: 2.4032\n","Epoch 3/10\n","550/550 [==============================] - 340s 618ms/step - loss: 2.3082 - val_loss: 2.2821\n","Epoch 4/10\n","550/550 [==============================] - 340s 618ms/step - loss: 2.1762 - val_loss: 2.1926\n","Epoch 5/10\n","550/550 [==============================] - 339s 617ms/step - loss: 2.0667 - val_loss: 2.1282\n","Epoch 6/10\n","550/550 [==============================] - 340s 618ms/step - loss: 1.9342 - val_loss: 2.0706\n","Epoch 7/10\n","550/550 [==============================] - 340s 618ms/step - loss: 1.8094 - val_loss: 2.0165\n","Epoch 8/10\n","550/550 [==============================] - 340s 618ms/step - loss: 1.6832 - val_loss: 1.9717\n","Epoch 9/10\n","550/550 [==============================] - 340s 619ms/step - loss: 1.5575 - val_loss: 1.9380\n","Epoch 10/10\n","550/550 [==============================] - 340s 618ms/step - loss: 1.4363 - val_loss: 1.9098\n"]}],"source":["# 모델 학습\n","history = []\n","epochs = 10\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","\n","history = model.fit(enc_train, \n","          dec_train, \n","          epochs=epochs,\n","          batch_size=256,\n","          validation_data=(enc_val, dec_val),\n","          verbose=1)"]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9t9PbnToJA1","executionInfo":{"status":"ok","timestamp":1642672894048,"user_tz":-540,"elapsed":379,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"72c4d60e-4ad1-4b31-b318-69bc30de2c90"},"id":"N9t9PbnToJA1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  133019    \n","                                                                 \n"," lstm (LSTM)                 multiple                  16941056  \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  33562624  \n","                                                                 \n"," dense (Dense)               multiple                  14345049  \n","                                                                 \n","=================================================================\n","Total params: 64,981,748\n","Trainable params: 64,981,748\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":null,"id":"2efd63d3","metadata":{"id":"2efd63d3"},"outputs":[],"source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야됨\n","    while True:\n","        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됨 \n","\n","        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n","        test_tensor = tf.concat([test_tensor, \n","                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n","\n","        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 함\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장"]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QsjcnTInQUUO","executionInfo":{"status":"ok","timestamp":1642672901925,"user_tz":-540,"elapsed":1139,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"31583c84-c705-4a79-e63a-6f07e3f9e791"},"id":"QsjcnTInQUUO","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i love you <end> '"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","execution_count":null,"id":"6211976d","metadata":{"id":"6211976d","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1642672901926,"user_tz":-540,"elapsed":6,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"67d63a79-f9c3-490f-916b-63a6e46426c3"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> he s a monster <end> '"]},"metadata":{},"execution_count":17}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> he\", max_len=20)"]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> he has\", max_len=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"xKcMEkTXpbG2","executionInfo":{"status":"ok","timestamp":1642672902515,"user_tz":-540,"elapsed":8,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"44066ef2-9059-4a0e-fb4f-48a442283487"},"id":"xKcMEkTXpbG2","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> he has a sucker <end> '"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> she\", max_len=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"U_gSWPDbpEvE","executionInfo":{"status":"ok","timestamp":1642672902516,"user_tz":-540,"elapsed":7,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"6522d3a2-9a81-4bd3-cb73-e90d3504647c"},"id":"U_gSWPDbpEvE","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> she s got me runnin round and round oh oh oh oh oh oh oh <end> '"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> she is\", max_len=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"rEyIi9lwpdyL","executionInfo":{"status":"ok","timestamp":1642672904316,"user_tz":-540,"elapsed":1132,"user":{"displayName":"박주현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03930445207013370552"}},"outputId":"6f7309f7-4f22-4907-e5a5-16aaff61eaa1"},"id":"rEyIi9lwpdyL","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> she is a sucker <end> '"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["# 회고\n","\n","순환신경망(RNN) 작동 방법을 제대로 이해하지 못해 이번 프로젝트를 완성하는 데 어려움이 있었다.  순환신경망을 정리하자면 모델 내에서 생성한 단어를 다시 입력으로 사용하는 순환적 특성이 있는 인공지능이다.  이런 특성을 이용해 자연어처리를 하는데 순서는 아래와 같다.  \n","\n","- 문장->필터링->토큰화(단어로 쪼갬)->벡터화(단어사전)->데이터셋 분리->모델 학습->평가    \n","\n","그리고 데이터가 커서 학습시킬 때 시간이 오래 걸렸었다. 그래서 epoch을 5로 하다가 10으로 늘려서 학습했더니, val_loss 값을 2.2 이하로 줄일 수 있는 모델을 만들 수 있었다. 완성된 문장도 여러 개 출력해보니 모델이 잘 학습된 것 같다.\n","\n","이번 프로젝트를 진행하면 RNN의 활용사례에 관심이 생겼다.   자연어 처리 뿐만 아니라 주식 가격이나 시계열 데이터 예측 등에도 활용될 수 있을 것 같다. "],"metadata":{"id":"Q1UfZye1oZcp"},"id":"Q1UfZye1oZcp"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"[E-04]LyricistAI.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}